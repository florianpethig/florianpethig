---
title: Publications
---



<div id="work-in-progress" class="section level2">
<h2>Work in Progress</h2>
<p>Pethig F, Hoehle H, Hui KL, Lanz A (2023). “<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3936971">Behavior toward newcomers and contributions to online communities</a>.”</p>
<p align="justify">
<font size="2">In this paper, we study whether and how behavior toward newcomers impacts their socialization outcomes in online communities, such as retention and quality of contributions. Intuitively, more positive interactions should help newcomers adjust to the new environment, but the effect could be driven by endogenous responses: people interested in the community have an intrinsic propensity to participate, while their posts receive more positive responses from existing members. By exploiting a natural experiment on a large deal-sharing platform, we find that an intervention that reminds people to be more considerate to newcomers causes newcomer deals to receive 54% more comments with a more positive sentiment. In turn, we find that newcomers are 4% more likely to post another deal, suggesting an increase in retention. However, we do not observe any effect on the quality of subsequent contributions. Our evidence suggests that the intervention merely caused a temporary shock to the first contributions of newcomers, but failed to improve learning or motivate greater effort. We draw implications for the design of socialization processes to help communities improve the retention and performance of newcomers.</font>
</p>
<p align="center">
<img src="/about/nudge.png" style="max-width:80%">
</p>
<p>Pethig F, Hoehle H, Hui KL, Lanz A (2023). “Unexpected monetary incentives and user-generated content on digital platforms.”</p>
<p align="justify">
<font size="2">Digital platforms frequently use monetary rewards to incentivize the production of user-generated content (UGC). We leverage data from deal-sharing platforms that award users who post the most popular contributions per day with a 10 EUR voucher. An important caveat of our setting is that the incentive scheme is not publicly announced by the platforms. Instead, recipients are informed by platform representatives, who disclose the reward and the rules of the scheme. Thus, the scheme endogenously sorts users into informed users who can compete for future rewards and uninformed users who remain unaware of the scheme. We show that although winning the reward increases the number of contributions, it causes informed users to downvote others’ contributions. Our findings suggest that incentive schemes might inadvertently increase competition among its users with potentially unintended negative effects.
</font>
</p>
<p align="center">
<img src="/about/unexpected.png" style="max-width:80%">
</p>
</div>
<div id="publications" class="section level2">
<h2>Publications</h2>
<p>Pethig F, Kroenung J (2023). “<a href="https://link.springer.com/content/pdf/10.1007/s10551-022-05071-8.pdf">Biased humans, (un) biased algorithms?</a>.” <em>Journal of Business Ethics</em>, <em>183</em>, 637-652.</p>
<p align="justify">
<font size="2">Previous research has shown that algorithmic decisions can reflect gender bias. The increasingly widespread utilization of algorithms in critical decision-making domains (e.g., healthcare or hiring) can thus lead to broad and structural disadvantages for women. However, women often experience bias and discrimination through human decisions and may turn to algorithms in the hope of receiving neutral and objective evaluations. Across three studies (n=1,107), we examine whether women’s receptivity to algorithms is affected by situations in which they believe that their gender identity might disadvantage them in an evaluation process. In Study 1, we establish, in an incentive-compatible online setting, that unemployed women are more likely to choose to have their employment chances evaluated by an algorithm if the alternative is an evaluation by a man rather than a woman. Study 2 generalizes this effect by placing it in a hypothetical hiring context, and Study 3 proposes that relative algorithmic objectivity, i.e.~the perceived objectivity of an algorithmic evaluator over and against a human evaluator, is a driver of women’s preferences for evaluations by algorithms as opposed to men. Our work sheds light on how women make sense of algorithms in stereotype-relevant domains and exemplifies the need to provide education for those at risk of being adversely affected by algorithmic decisions. Our results have implications for the ethical management of algorithms in evaluation settings. We advocate for improving algorithmic literacy so that evaluators and evaluatees (e.g., hiring managers and job applicants) can acquire the abilities required to reflect critically on algorithmic decisions.
</font>
</p>
<p align="center">
<img src="/about/bias.png" style="max-width:60%">
</p>
<p>Pethig F, Kroenung J, Noeltner M (2021). “<a href="https://www.sciencedirect.com/science/article/pii/S0740624X20303245">A stigma power perspective on digital government service avoidance</a>.” <em>Government Information Quarterly</em>, <em>38</em>(2), 101545.</p>
<p align="justify">
<font size="2">The digital-by-default policy for government services implemented in many European countries can pose challenges to marginalized citizens, such as people with disabilities. Prior research on electronic inclusion and the digital divide has mainly considered technology-related concerns, such as Internet anxiety, preventing people with disabilities from using digital government services. Yet, these concerns may insufficiently account for the fact that people with disabilities may suspect that governments provide new services only to reduce costs and forgo the need for more meaningful social change. Therefore, we draw from stigma power theory to understand how perceptions of stereotyping and discrimination contribute to the avoidance of digital government services among people with disabilities. Our results indicate that overcoming underutilization of digital government services among people with disabilities requires a holistic approach by addressing technology-related as well as stigma-related concerns.
</font>
</p>
<p>Pethig F, Kroenung J (2019). “<a href="https://aisel.aisnet.org/jais/vol20/iss10/5/">Specialized information systems for the digitally disadvantaged</a>.” <em>Journal of the Association for Information Systems</em>, <em>20</em>(10), 247-265.</p>
<p align="justify">
<font size="2">A number of specialized information systems for the digitally disadvantaged (SISD) have been developed to offset the limitations of people less able to participate in the information society. However, contributions from social identity theory and social markedness theory indicate that SISD can activate a stigmatized identity and thus be perceived unfavorably by their target audience. We identify two mechanisms by which functional limitations affect a digitally disadvantaged person’s adoption decision: (1) adoption decision as shaped through technology perceptions (i.e., perceived usefulness, perceived ease of use, and perceived access barriers), and (2) adoption decision as shaped through marked status awareness (i.e., stigma consciousness). We test our contextualized research model on digitally disadvantaged users with physical and/or sensory disabilities. Results of our mediation analysis show that the individuals who have the most to gain from SISD use (i.e., those with greater perceived functional limitations) are doubly disadvantaged: as a group, they find it more challenging to use SISD and are also more sensitive to the fear of being marked as disadvantaged or vulnerable.
</font>
</p>
</div>
